{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# 환경 정의\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(4)  # 0: 위, 1: 아래, 2: 좌, 3: 우\n",
    "        # self.observation_space = spaces.Discrete(25)  # 5x5 그리드\n",
    "        self.state = None\n",
    "        self.grid = np.zeros((5, 5))\n",
    "        self.grid[4, 4] = 1  # 목표 위치\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state_to_index(self.state)\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0 and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 1 and x < 4:\n",
    "            x += 1\n",
    "        elif action == 2 and y > 0:\n",
    "            y -= 1\n",
    "        elif action == 3 and y < 4:\n",
    "            y += 1\n",
    "\n",
    "        self.state = (x, y)\n",
    "        done = x == 4 and y == 4\n",
    "        reward = 1 if done else 0\n",
    "        return self.state_to_index(self.state), reward, done, {}\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        grid = self.grid.copy()\n",
    "        grid[self.state] = 2\n",
    "        for row in grid:\n",
    "            print(\" \".join(str(int(x)) for x in row))\n",
    "        print()\n",
    "\n",
    "    def state_to_index(self, state):\n",
    "        return state[0] * 5 + state[1]  # 좌표를 선형 인덱스로 변환\n",
    "    \n",
    "# # Q-테이블 초기화 및 파라미터 설정\n",
    "# q_table = np.zeros((25, 4))\n",
    "# alpha = 0.1\n",
    "# gamma = 0.6\n",
    "# epsilon = 0.1\n",
    "\n",
    "# # 환경 인스턴스 생성 및 학습\n",
    "# env = CustomEnv()\n",
    "# for i in range(1000):\n",
    "#     state = env.reset()\n",
    "\n",
    "#     done = False\n",
    "#     while not done:\n",
    "#         if random.uniform(0, 1) < epsilon:\n",
    "#             action = env.action_space.sample()\n",
    "#         else:\n",
    "#             action = np.argmax(q_table[state])\n",
    "\n",
    "#         print(action)\n",
    "\n",
    "#         next_state, reward, done, _ = env.step(action)\n",
    "#         print(next_state)\n",
    "#         old_value = q_table[state, action]\n",
    "#         next_max = np.max(q_table[next_state])\n",
    "#         new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "#         q_table[state, action] = new_value\n",
    "#         state = next_state\n",
    "\n",
    "# # 환경 테스트\n",
    "# state = env.reset()\n",
    "# env.render()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = np.argmax(q_table[state])\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 인스턴스 생성 및 초기화\n",
    "env = CustomEnv()\n",
    "state = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = env.action_space.sample()  # 무작위 액션\n",
    "    state, reward, done, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-테이블 초기화 및 파라미터 설정\n",
    "q_table = np.zeros((25, 4))\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# 환경 인스턴스 생성 및 학습\n",
    "env = CustomEnv()\n",
    "for i in range(1000):\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])\n",
    "\n",
    "        print(action)\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        print(next_state)\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        state = next_state\n",
    "\n",
    "# # 환경 테스트\n",
    "# state = env.reset()\n",
    "# env.render()\n",
    "# done = False\n",
    "# while not done:\n",
    "#     action = np.argmax(q_table[state])\n",
    "#     state, reward, done, _ = env.step(action)\n",
    "#     env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 환경 테스트\n",
    "state = env.reset()\n",
    "print(state)\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    print(action)\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# 환경 정의\n",
    "class CustomEnv(gym.Env):\n",
    "    metadata = {'render.modes': ['console']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomEnv, self).__init__()\n",
    "        self.action_space = spaces.Discrete(4)  # 0: 위, 1: 아래, 2: 좌, 3: 우\n",
    "        # self.observation_space = spaces.Discrete(25)  # 5x5 그리드\n",
    "        self.state = (0, 0)\n",
    "        self.grid = np.zeros((5, 5))\n",
    "        self.grid[4, 4] = 1  # 목표 위치\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = (0, 0)\n",
    "        return self.state  # 좌표 반환\n",
    "\n",
    "    def step(self, action):\n",
    "        x, y = self.state\n",
    "        if action == 0 and x > 0:\n",
    "            x -= 1\n",
    "        elif action == 1 and x < 4:\n",
    "            x += 1\n",
    "        elif action == 2 and y > 0:\n",
    "            y -= 1\n",
    "        elif action == 3 and y < 4:\n",
    "            y += 1\n",
    "\n",
    "        self.state = (x, y)\n",
    "        done = x == 4 and y == 4\n",
    "        reward = 1 if done else 0\n",
    "        return self.state, reward, done, {}\n",
    "\n",
    "    def render(self, mode='console'):\n",
    "        grid = self.grid.copy()\n",
    "        grid[self.state] = 2\n",
    "        for row in grid:\n",
    "            print(\" \".join(str(int(x)) for x in row))\n",
    "        print()\n",
    "\n",
    "# Q-테이블 초기화 및 파라미터 설정\n",
    "q_table = np.zeros((25, 4))\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# 환경 인스턴스 생성 및 학습\n",
    "env = CustomEnv()\n",
    "for i in range(1000):\n",
    "    state = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state[0] * 5 + state[1]])\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        old_value = q_table[state[0] * 5 + state[1], action]  # 직접 좌표를 인덱스로 변환\n",
    "        next_max = np.max(q_table[next_state[0] * 5 + next_state[1]])\n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state[0] * 5 + state[1], action] = new_value\n",
    "        \n",
    "        if not done:  # 에피소드가 종료되지 않은 경우에만 상태를 업데이트합니다.\n",
    "            state = next_state\n",
    "\n",
    "\n",
    "# 환경 테스트\n",
    "state = env.reset()\n",
    "env.render()\n",
    "done = False\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state[0] * 5 + state[1]])\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    env.render()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DQN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
